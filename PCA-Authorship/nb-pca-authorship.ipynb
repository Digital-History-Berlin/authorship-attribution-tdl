{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionsreduzierung zur Identifikation von Autorenschaft: Einsatz der Principal Component Analysis\n",
    "\n",
    "Die Principal Component Analysis (PCA), zu deutsch Hauptkomponentenanalyse, ist ein Algorithmus zur Dimensionsreduzierung, der neben anderen Einsatzgebieten zunehmend auch Einzug in die Analyse textueller Daten gefunden hat. [^fn1] Damit der Computer mit textuellen Daten rechnen kann, müssen diese in numerischen Werten repräsentiert sein. Textkorpora werden dazu etwa über die Auszählung von Worthäufigkeiten in einen Wortvektorraum übertragen: Für jedes Dokument eines Textkorpus wird ein Vektor erstellt, der beispielsweise Worthäufigkeiten der Wörter, die in dem Dokument enthalten sind, repräsentiert. Da Textkorpora viele Dokumente umfassen können und damit ein aus sehr vielen Wörtern bestehendes Vokabular bilden, besteht der Wortvektorraum aus Vektoren mit sehr vielen Dimensionen. Unter einer Dimension ist hier die Spalte einer Document-Term-Matrix (DTM) gemeint, die alle Häufigkeiten zu einem Wort des Vokabulars eines Korpus enthält. Die Zeilen der DTM enthalten die Vektoren mit den Worthäufigkeiten, der Wörter, die in einem Dokument enthalten sind. Allerdings lassen sich für einen menschlichen Betrachter nur zwei bzw. drei Dimensionen visualisieren. Um mehrere Dimensionen auf nur zwei oder drei Dimensionen zu bringen, können Verfahren der Dimensionsreduzierung angewandt werden. Eines dieser Verfahren ist die Principal Component Analysis, die nachfolgend veranschaulicht werden soll.\n",
    "\n",
    "Die im Folgenden genutzten [Daten](https://doi.org/10.5281/zenodo.3560761) [^fn2] sowie der Code, der leicht angepasst verwendet wird, basiert konkret auf einem [Unterkapitel](https://www.humanitiesdataanalysis.org/stylometry/notebook.html#principal-component-analysis) sowie auf dem einführenden Kapitel [Stylometry and the Voice of Hildegard](https://www.humanitiesdataanalysis.org/stylometry/notebook.html#) aus der praktischen Einführung Humanities Data Analysis von Karsdorp, Kestemont und Riddell. [^fn3] Als historisches Fallbeispiel dienen mittelalterliche Quellen, die entweder von Hildegard von Bingen oder von einem ihrer Sekretäre verfasst wurden. Ein informatives Video zu diesem Fall steht [hier](https://vimeo.com/70881172) zur Verfügung. Das Video wurde von den Forschenden erstellt, die diese unklare Lage bei der Autorenschaft der lateinischen Quellen aus dem 12. Jahrhundert wissenschaftlich untersucht haben. [^fn4]\n",
    "\n",
    "[^fn1]: Jonathon Shlens, A Tutorial on Principal Component Analysis, in: arXiv:1404.1100, (3. April 2014), http://arxiv.org/abs/1404.1100.\n",
    "\n",
    "[^fn2]: Folgert Karsdorp, Mike Kestemont, Allen Roddell, Supplemental Materials for \"Humanities Data Analysis\" [Data set]. Zenodo, https://doi.org/10.5281/zenodo.3560761.\n",
    "\n",
    "[^fn3]: Folgert Karsdorp, Mike Kestemont, Allen Roddell, Humanities Data Analysis. Case Studies with Python, Princeton 2021, S. 248-280, hier 252-254, https://www.humanitiesdataanalysis.org/index.html\n",
    "\n",
    "[^fn4]: Mike Kestemont, Sara Moens, Jeroen Deploige, Collaborative authorship in the twelfth century: A stylometric study of Hildegard of Bingen and Guibert of Gembloux, in: Digital Scholarship in the Humanities 30, Nr. 2 (2015), S. 199–224, https://doi.org/10.1093/llc/fqt063\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.decomposition\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helferfunktion\n",
    "\n",
    "### Zum Laden der Daten\n",
    "\n",
    "Im Ordner 'texts' befinden sich drei Textkorpora: Zum einen von Hildegard von Bingen, von ihrem Sekretär Guibert von Gembloux sowie als unabhängige Vergleichsgröße von einem weiteren Zeitgenossen, Bernard von Clairvaux. Die lateinischen Textkorpora liegen bereits in lemmatisierter Form vor. Da zur stilometrischen Analyse die zu vergleichenden Texte eine einheitliche Länge aufweisen sollen, bringt die nachfolgende Funktion die Textkorpora auf den entsprechenden Umfang. Die Länge wird bestimmt vom kürzesten der zu untersuchenden Texte. Die im Unterordner 'test' abgelegten Dateien enthalten die zu prüfenden Quellen mit den unklaren Autor:innenzuschreibungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory(directory, max_length):\n",
    "    documents, authors, titles = [], [], []\n",
    "    for filename in os.scandir(directory):\n",
    "        if not filename.name.endswith('.txt'):\n",
    "            continue\n",
    "        author, _ = os.path.splitext(filename.name)\n",
    "\n",
    "        with open(filename.path) as f:\n",
    "            contents = f.read()\n",
    "        lemmas = contents.lower().split()\n",
    "        start_idx, end_idx, segm_cnt = 0, max_length, 1\n",
    "\n",
    "        # extract slices from the text:\n",
    "        while end_idx < len(lemmas):\n",
    "            documents.append(' '.join(lemmas[start_idx:end_idx]))\n",
    "            authors.append(author[0])\n",
    "            title = filename.name.replace('.txt', '').split('_')[1]\n",
    "            titles.append(f\"{title}-{segm_cnt}\")\n",
    "\n",
    "            start_idx += max_length\n",
    "            end_idx += max_length\n",
    "            segm_cnt += 1\n",
    "\n",
    "    return documents, authors, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden der Daten\n",
    "\n",
    "Die Textkorpora werden in Abschnitte (chunks) mit einer Länge von 10.000 Tokens gebracht. In den zurückgegebenen Listen befinden sich die Quellentexte als string (documents), der erste Buchstabe des:r Autor:in (authors) als Label für die spätere Klassifizierung und Visualisierung sowie der restliche Teil des Dateinamens (title)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, authors, titles = load_directory('data/texts', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Anzahl der Dokumente: {len(documents)}')\n",
    "print(authors[:5])\n",
    "print(titles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erstellen des Vokabulars\n",
    "\n",
    "Computer verarbeiten Zahlen - daher müssen Texte in numerische Werte umgewandelt werden: Für jedes jedes Dokument in einem Textkorpus kann ein Vektor erstellt werden, der das Dokument repräsentiert. Bei stilometrischen Analysen wurde herausgefunden, dass Funktionswörter, insbesondere Stoppwörter, als starkes Signal zur Bestimmung von Autorenschaft herangezogen werden können. Funktionswörter werden von Autor:innen unbewusst in bestimmten Mustern genutzt, dadurch stellen sie eine sehr gute Signatur dar, um Autor:innen zu identifizieren. Für den vorliegenden Fall wurde eine Liste erstellt, die 65 Funktionswörter umfasst. Diese Liste wird zunächst aus der Datei wordlist.txt in die Variable vocab eingelesen. Dies ist das Vokabular, welches bei der weiteren stilometrischen Untersuchung zur Authorship Attribution genutzt wird. Mehr zum Hintergrund der Auswahl sowie zur Rolle von Funktionswörtern bietet ein [Unterkapitel](https://www.humanitiesdataanalysis.org/stylometry/notebook.html#function-words) in der Einführung Humanities Data Analysis von Karsdorp, Kestemont und Riddell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [l.strip() for l in open('data/wordlist.txt') if not l.startswith('#') and l.strip()][:65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vektorisierung, Normalisierung und Standardisierung\n",
    "\n",
    "Die Python-Bibliothek [scikit-learn](https://scikit-learn.org/stable/#) bietet viele Funktionalitäten und Algorithmen für die Datenanalyse sowie für Machine Learning. Die Objektklasse [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer) ermöglicht es, die Häufigkeiten von Wörtern in einem Textkorpus zu ermitteln. Zugleich übernimmt sie auch die Tokenisierung von Texten, die hier im nachfolgenden Code über einen als Parameter übergebenen regulären Ausdruck erfolgt. Im Ergebnis erstellt der CountVectorizer auf der Basis des übergebenen Vokabulars eine Document-Term-Matrix mit Wordhäufigkeiten.\n",
    "\n",
    "Beim Normalisieren wird jedes Element des Dokumentvektors durch die Länge des Vektors, d.h. durch die Summer aller Elemente des Vektors, geteilt. Zur Anwendung kommt wiederum eine [Methode zur Normalisierung](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html?highlight=normalize#sklearn.preprocessing.normalize) aus [scikit-learn](https://scikit-learn.org/stable/#), die im konkreten Fall mit L1 normalisiert, was der beschriebenen Division durch die Vektorlänge entspricht.\n",
    "\n",
    "Eine Standardisierung der Werte in der Document-Term-Matrix übernimmt die Objektklasse [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler) aus scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = text.CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", vocabulary=vocab)\n",
    "\n",
    "v_documents = vectorizer.fit_transform(documents).toarray()\n",
    "print(v_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_documents = preprocessing.normalize(v_documents.astype(np.float64), 'l1')\n",
    "scaler = preprocessing.StandardScaler()\n",
    "print(v_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_documents = scaler.fit_transform(v_documents)\n",
    "\n",
    "print(f'N={v_documents.shape[0]} Dokumente mit '\n",
    "      f'V={v_documents.shape[1]} Features (= Dimensionen).')\n",
    "\n",
    "print(v_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Durchführen der PCA\n",
    "\n",
    "Eine Dimensionsreduzierung mittels PCA ermöglicht es, ein Datenset mit vielen Dimensionen auf weniger Dimensionen zu bringen, wobei gleichzeitig die größtmögliche Annäherung an die ursprünglichen Daten erhalten bleibt. Es sollte daher klar sein, dass es sich bei den dimensionsreduzierten Daten nur um eine sich den Originaldaten annähernde Zusammenfassung handelt.\n",
    "\n",
    "Die PCA ermittelt Korrelationen zwischen den Daten, um daraus neue Informationen zu generieren, indem korrelierenden Daten zusammengefasst werden. Betrachtet man Autor:innenstile, dann schließen sich beispielweise die Verwendung von bestimmten Artikeln und unbestimmten Artikeln aus: Neigt eine Person dazu, mehr bestimmte Artikel zu nutzen, dann sind in Texten weniger unbestimmte Artikel zu finden. Aus dieser Information lässt sich eine neue zusammengefasste Information gewinnen und zwei Dimensionen werden zu einer zusammengefasst. \n",
    "\n",
    "Mit zwei Zeilen Code werden die 65 Dimensionen, die dem Vokabular mit den 65 ausgewählten Funktionswörtern entspricht, auf zwei Dimensionen reduziert. Zum Einsatz kommt die Objektklasse [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA) aus [scikit-learn](https://scikit-learn.org/stable/#), die die notwendigen Berechnungen durchführt. Der Parameter *n_components* gibt die Anzahl der Dimensionen an, auf die der Algorithmus die Daten reduzieren soll. Die reduzierte Document Term Matrix umfasst zwei Hauptkomponenten (principal components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "documents_proj = pca.fit_transform(v_documents)\n",
    "\n",
    "print(f'Die ursprüngliche DTM mit \\n {v_documents.shape[0]} Dokumenten umfasst {v_documents.shape[1]} Dimensionen.\\n')\n",
    "print(f'Die dimensionsreduzierte DTM mit \\n {documents_proj.shape[0]} Dokumenten umfasst {documents_proj.shape[1]} Dimensionen.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ursprüngliche DTM:')\n",
    "print(v_documents[:2])\n",
    "print('\\ndimensionsreduzierte DTM:')\n",
    "print(documents_proj[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot der dimensionsreduzierten Dokument-Term-Matrix\n",
    "\n",
    "Die zweidimensionalen Daten können in einem Koordinatensystem (scatterplot) visualisiert werden. Die erste Hauptkomponente wird auf der x-Achse, die zweite Hauptkomponente auf der y-Achse abgebildet. In der resultierenden Visualisierung bilden die Datenpunkte der dimensionsreduzierten Texte sehr deutliche Cluster. Auf diese Weise werden unterschiedliche Schreibstile der Autor:innen sichtbar.\n",
    "\n",
    "### Helferfunktion zum Erstellen eines Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(document_proj, var_exp, labels):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    x1, x2 = documents_proj[:, 0], documents_proj[:, 1]\n",
    "    ax.scatter(x1, x2, facecolors='none')\n",
    "                    \n",
    "    for p1, p2, author in zip(x1, x2, labels):\n",
    "        color = 'red' if author not in ('H', 'G', 'B') else 'black'\n",
    "        ax.text(p1, p2, \n",
    "                     author, \n",
    "                     ha='center',\n",
    "                     color=color, \n",
    "                     va='center', \n",
    "                     fontsize=12)\n",
    "\n",
    "    # add variance information to the axis labels:\n",
    "    ax.set_xlabel(f'PC1 ({var_exp[0] * 100:.2f}%)')\n",
    "    ax.set_ylabel(f'PC2 ({var_exp[1] * 100:.2f}%)')\n",
    "    ax.set_xlim(left=None,right=11)\n",
    "    ax.set_ylim(bottom=None, top=8)\n",
    "\n",
    "    ax.set_title('Visualisierung der dimensionsreduzierten Textdaten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "documents_proj = pca.fit_transform(v_documents)\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "\n",
    "plot_pca(documents_proj, var_exp, authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Durchschnittlich erklärte Varianz\n",
    "\n",
    "Nach Durchführung einer PCA lässt sich angeben, welche Hauptkomponente wie genau eine Zusammenfassung der Originaldaten anbietet, wenn man jede Hauptkomponente für sich betrachtet. Die Hauptkomponenten werden danach geordnet, wie gut sie sich den Originaldaten annähern. Über die Methode *explained_variance_ratio_* können die entsprechenden Werte abgerufen und visualisiert werden. Dazu wird eine neue PCA durchgeführt allerdings ist nun die Anzahl der zu reduzierenden Dimensionen mit *n_components* gleich 36 angegeben, der Anzahl der vorhandenen Dokumente. Visualisiert wird, wie gut jede Hauptkomponente für sich die Originaldaten abbildet und wie gut die Hauptkomponenten diese kumulativ zusammenfassen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA(n_components=36)\n",
    "pca.fit(v_documents)\n",
    "\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.bar(range(36), var_exp, alpha=0.5, align='center',\n",
    "        label='indivuduell erklärte Varianz')\n",
    "\n",
    "ax.step(range(36), cum_var_exp, where='mid',\n",
    "         label='kumulativ erklärte Varianz')\n",
    "\n",
    "ax.axhline(0.05, ls='dotted', color=\"black\")\n",
    "ax.set(ylabel='Durchschnittlich erklärte Varianz', xlabel='Hauptkomponente')\n",
    "ax.legend(loc='center right')\n",
    "ax.set_title('Veranschaulichung zur PCA');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen und Verarbeiten der Testdaten\n",
    "\n",
    "Um ein Befund für eine Authorship Attribution zu generieren, müssen nun die zu untersuchenden, unklaren Textdaten dem Korpus der bekannten Dokumente hinzugefügt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs, test_authors, test_titles = load_directory('data/texts/test', 3301)\n",
    "\n",
    "v_test_docs = vectorizer.transform(test_docs)\n",
    "v_test_docs = preprocessing.normalize(v_test_docs.astype(float), norm='l1')\n",
    "v_test_docs = scaler.transform(v_test_docs.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisieren der bekannten und unklaren Quellendokumente in einem Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = preprocessing.scale(np.vstack((v_documents, v_test_docs)))\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "documents_proj = pca.fit_transform(all_documents)\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "\n",
    "plot_pca(documents_proj, var_exp, list(authors) + test_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnis\n",
    "\n",
    "In der Visualisierung der dimensionsreduzierte Textdaten stehen  die Quellendokumente mit einer unklaren Autor:innenzuschreibung dem Cluster von Guibert von Gembloux nahe. Dessen Autorenschaft ist somit wahrscheinlicher als eine Autorenschaft von Hildegard von Bingen"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88af0ede569fe5c04062aeb490d0f03883bc9347bee31fbe08a2787a335dbcd5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
