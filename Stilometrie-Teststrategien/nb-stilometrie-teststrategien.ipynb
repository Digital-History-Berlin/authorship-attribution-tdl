{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stilometrie: basale Teststrategien und grundlegende Ansätze\n",
    "\n",
    "Das Notebook zeigt grundlegende und einfache Teststrategien, um über stilometrische Ansätze, Autor:innen zu identifizieren.\n",
    "\n",
    "Folgende Verfahren werden getestet:\n",
    "\n",
    "- Häufigkeitsverteilung der Wortlänge\n",
    "- Häufigkeitsverteilung der Stoppwörter\n",
    "- Häufigkeitsverteilung der Wortarten (Part-of-Speech)\n",
    "- Vergleich der am häufigsten verwendeten Wörter\n",
    "- Jaccard Similarity\n",
    "\n",
    "Der nachfolgende Python-Code basiert auf zwei Vorlagen, der für das vorliegende Notebook angepasst wurde:\n",
    "\n",
    "Lee Vaughan, Real World Python: A Hacker’s Guide to Solving Problems with Code, San Francisco 2021, S. 27-50. Der zum Buch gehörige Code unter: https://github.com/rlvaugh/Real_World_Python/blob/master/Chapter_2/stylometry.py\n",
    "\n",
    "François Dominic Laramée, Introduction to stylometry with Python, in: Programming Historian 7 (2018), https://doi.org/10.46430/phen0078."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folgende Daten müssen einmalig nach der Installation von NLTK heruntergeladen werden:\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helferfunktionen \n",
    "\n",
    "### Einlesen der Dateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_string(filename):\n",
    "    '''Read a text file and return a string.'''\n",
    "\n",
    "    with open(f'data/{filename}') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen eines Dictionaries mit tokenisierten Wörtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_dict(strings_by_author):\n",
    "    \"\"\"Return dictionary of tokenized words by corpus by author.\"\"\"\n",
    "    words_by_author = dict()\n",
    "    for author in strings_by_author:\n",
    "        tokens = nltk.word_tokenize(strings_by_author[author], language='german')\n",
    "        words_by_author[author] = ([token.lower() for token in tokens if token.isalpha()])\n",
    "    return words_by_author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kürzesten Corpus finden\n",
    "\n",
    "Bei den stilometrischen Tests sollten die zu vergleichenden Texte die gleiche Länge aufweisen. Es ist möglich mit Textausschnitten (chunks) zu arbeiten, dem sog. Chunking. Es können die relativen Häufigkeiten der Wörter in einem Text genutzt werden, um die Häufigkeiten zu normalisieren. Oder alle Texte der zu vergleichende Korpora können auf die Länge des kürzesten Textes eines Korpus gebracht werden (truncating). Der letztgenannte Ansatz wird nachfolgend angewandt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shortest_corpus(words_by_author):\n",
    "    \"\"\"Return length of shortest corpus.\"\"\"\n",
    "    word_count = []\n",
    "    for author in words_by_author:\n",
    "        word_count.append(len(words_by_author[author]))\n",
    "        print('Anzahl der Tokens für {} = {}\\n'.\n",
    "              format(author, len(words_by_author[author])))\n",
    "    len_shortest_corpus = min(word_count)\n",
    "    print('Länge des kürzesten Korpus = {}\\n'.format(len_shortest_corpus))        \n",
    "    return len_shortest_corpus  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Texte in ein Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings_by_author = dict()\n",
    "strings_by_author['goethe'] = text_to_string('goethe.txt')\n",
    "strings_by_author['humboldt'] = text_to_string('humboldt.txt')\n",
    "strings_by_author['unknown'] = text_to_string('unknown.txt')\n",
    "\n",
    "print(strings_by_author['goethe'][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisierung und Bestimmung des kürzesten Korpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_by_author = make_word_dict(strings_by_author)\n",
    "len_shortest_corpus = find_shortest_corpus(words_by_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwenden der Teststrategien\n",
    "\n",
    "### Häufigkeiten der Wortlängen\n",
    "\n",
    "Durch das Auszählen und Visualisieren von Häufigkeiten der Wortlängen in einem ausreichend großen Textkorpus lässt sich mit Hilfe eines Liniendiagrammes eine charakteristische Kurve für Autor:innen eines Textkorpus veranschaulichen. Die Methode, Häufigkeiten von Wortlängen auszuzählen, wird schon länger angewandt, ist aber im Vergleich zu den aktuellen, viel elaborierten stilometrischen Methoden recht grob. Dennoch lassen sich mit diesem Ansatz durchaus erste Befunde gewinnen. \n",
    "\n",
    "Thomas C. Mendenhall, The Characteristic Curves of Composition, in: Science, vol. 9, no. 214 (Mar. 11, 1887), S. 237-249, https://doi.org/10.1126/science.ns-9.214s.237 / https://zenodo.org/record/1448355#.YlfoSNPP23A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linestyles für die Plots\n",
    "LINES = ['-', ':', '--'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_length_test(words_by_author, len_shortest_corpus):\n",
    "    \"\"\"Plot word length freq by author, truncated to shortest corpus length.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    for i, author in enumerate(words_by_author):\n",
    "        word_lengths = [len(word) for word in words_by_author[author][:len_shortest_corpus] if len(word) > 2]\n",
    "        by_author_length_freq_dist = nltk.FreqDist(word_lengths).most_common(15)\n",
    "\n",
    "        ser_fdist = pd.Series(dict(by_author_length_freq_dist)) # Konvertieren in pandas-Series\n",
    "        ser_fdist.sort_index().plot(ax=ax, \n",
    "                                    legend=True,\n",
    "                                    linestyle=LINES[i],\n",
    "                                    label=author,\n",
    "                                    title='Vergleich: Häufigkeiten der Wortlänge',\n",
    "                                    grid=True,\n",
    "                                    xlabel='Wortlänge in Buchstaben',\n",
    "                                    ylabel='Häufigkeit',\n",
    "                                    xticks=range(3,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_length_test(words_by_author, len_shortest_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Häufigkeiten der Stoppwörter\n",
    "\n",
    "Normalerweise werden Stoppwörter bei vielen Anwendungsszenarien des Natural Language Processing (NLP) aus einem Textkorpus entfernt, da sie nicht bedeutungstragend sind, daher wenig Information bieten und im Gegenteil ein stärkes Rauschen bei digitalen Textanalyseverfahren verursachen. Im Falle der stilometrischen Untersuchung von Texten hingegen erweisen sich die Stoppwörter als starkes Signal zur Bestimmung von Autorenschaft. Da Stoppwörter unbewusst und von Autor:innen in bestimmten Mustern genutzt werden, stellen sie eine gute Signatur dar, um Autor:innen zu identifzieren. Zwar lässt sich im nachfolgenden Diagramm eine Tendenz ablesen, jedoch nutzen ausgefeiltere stilometrischen Methoden Stoppwörter noch wesentlich effizienter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stopwords_test(words_by_author, len_shortest_corpus):\n",
    "    \"\"\"Plot stopwords freq by author, truncated to shortest corpus length.\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "    AXES = [ax, ax.twiny(), ax.twiny()]\n",
    "    COLORS = plt.get_cmap(\"tab10\")\n",
    "    SPINES_POSITION = [-.0, -.3, -.5] \n",
    "\n",
    "    stop_words = set(stopwords.words('german')) \n",
    "    print('Number of stopwords = {}\\n'.format(len(stop_words)))\n",
    "\n",
    "    for i, author in enumerate(words_by_author):\n",
    "\n",
    "        stopwords_by_author = [word for word in words_by_author[author][:len_shortest_corpus] if word in stop_words]\n",
    "        stopwords_by_author_freq_dist = nltk.FreqDist(stopwords_by_author).most_common(50)\n",
    "        author_fdist = pd.Series(dict(stopwords_by_author_freq_dist))\n",
    "        author_fdist.plot(ax=AXES[i], \n",
    "                        label=author,\n",
    "                        legend=False,\n",
    "                        linestyle=LINES[i],\n",
    "                        title='Vergleich: Häufigkeiten der Stoppwörter',\n",
    "                        color=COLORS(i),\n",
    "                        ylabel='Häufigkeiten',\n",
    "                        grid=True)\n",
    "        AXES[i].spines['top'].set_position(('axes', SPINES_POSITION[i]))\n",
    "        AXES[i].set_xticks(range(50))\n",
    "        AXES[i].set_xticklabels(author_fdist.index, rotation=90, color=COLORS(i))\n",
    "        AXES[i].spines['right'].set_visible(False)\n",
    "\n",
    "    # add legend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stopwords_test(words_by_author, len_shortest_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Häufigkeiten der Wortarten (Part-of-Speech)\n",
    "\n",
    "Ebenso lassen sich durch ein einfaches Auszählen der Wortarten ein weiterer Befund zum Stil eines:r Autors:in generieren, der im vorliegenden Fall allerdings nicht sehr deutlich ausfällt. Eine alphabetische List der Abkürzungen zu den verwendeten Tags ist [hier](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) zu finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parts_of_speech_test(words_by_author, len_shortest_corpus):\n",
    "    \"\"\"Plot author use of parts-of-speech such as nouns, verbs, adverbs,etc.\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "    AXES = [ ax, ax.twiny(), ax.twiny()]\n",
    "    COLORS = plt.get_cmap(\"tab10\")\n",
    "    SPINES_POSITION = [ -.0, -.3, -.5]\n",
    "\n",
    "    for i, author in enumerate(words_by_author):\n",
    "\n",
    "        pos_by_author = [pos[1] for pos in nltk.pos_tag(words_by_author[author][:len_shortest_corpus])] \n",
    "        pos_by_author_freq_dist = nltk.FreqDist(pos_by_author).most_common(25)\n",
    "        author_fdist = pd.Series(dict(pos_by_author_freq_dist))\n",
    "        author_fdist.plot(ax=AXES[i], # oder sort_index() \n",
    "                        label=author,\n",
    "                        legend=False,\n",
    "                        linestyle=LINES[i],\n",
    "                        title='Vergleich: Häufigkeiten nach Part-of-Speech-Tagging',\n",
    "                        color=COLORS(i),\n",
    "                        ylabel='Häufigkeiten',\n",
    "                        grid=False)\n",
    "        AXES[i].spines['top'].set_position(('axes', SPINES_POSITION[i]))\n",
    "        AXES[i].set_xticks(range(len(author_fdist.index)))\n",
    "        AXES[i].set_xticklabels(author_fdist.index, rotation=90, color=COLORS(i))\n",
    "        AXES[i].spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parts_of_speech_test(words_by_author, len_shortest_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich der am häufigsten verwendeten Wörter\n",
    "\n",
    "In der nächsten Teststrategie wird die Distanz zwischen den Vokabularien unterschiedlicher Textkorpora mittels der Chi-Square Methode gemessen. Verglichen werden die Mengen der einmalig im jeweiligen Korpus auftretenden Wörter. Um die Distanz bzw. die Ähnlichkeit der Texte zu eruieren, wird gemessen, wie sehr sich die Anzahl der einzelnen Wörter der Vergleichskorpora unterscheiden. Je ähnlicher die Vokabularien sind, umso wahrscheinlicher ist es, dass ein:e Autor:in die beiden verglichenen Texte verfasst hat. Dabei wird davon ausgegangen, dass sich das Vokabular eines:r Autor:in und die Wortverwendung vergleichweise konstant ist. Das Vorgehen wird in einem Abschnitt der Programming Historian-Lesson zur Stilometrie [hier](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#second-stylometric-test-kilgariffs-chi-squared-method) näher beschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_test(words_by_author):\n",
    "    \"\"\"Compare author vocabularies using the Chi Squared statistical test.\"\"\"\n",
    "    \n",
    "    chisquared_by_author = dict()\n",
    "    for author in words_by_author:\n",
    "        if author != 'unknown': \n",
    "            # Combine corpus for author & unknown & find 1000 most-common words.\n",
    "            combined_corpus = (words_by_author[author] +\n",
    "                               words_by_author['unknown'])\n",
    "            author_proportion = (len(words_by_author[author])/\n",
    "                                 len(combined_corpus))\n",
    "            combined_freq_dist = nltk.FreqDist(combined_corpus)\n",
    "            most_common_words = list(combined_freq_dist.most_common(1000))\n",
    "            chisquared = 0\n",
    "\n",
    "            # Calculate observed vs. expected word counts.\n",
    "            for word, combined_count in most_common_words:\n",
    "                observed_count_author = words_by_author[author].count(word)\n",
    "                expected_count_author = combined_count * author_proportion\n",
    "                chisquared += ((observed_count_author -\n",
    "                                expected_count_author)**2 /\n",
    "                               expected_count_author)\n",
    "                chisquared_by_author[author] = chisquared    \n",
    "            print('Chi-squared für {} = {:.1f}'.format(author, chisquared))\n",
    "            \n",
    "\n",
    "    most_likely_author = min(chisquared_by_author, key=chisquared_by_author.get)\n",
    "    print(f'Der wahrscheinlichste Autor nach Vergleich des Vokabulars: {most_likely_author}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_test(words_by_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berechnung der Jaccard-Metrik\n",
    "\n",
    "Auch mit dieser Metrik werden Distanz bzw. Ähnlichkeit von Textkorpora bestimmt. Der Wert wird errechnet, indem die Schnittmenge zweier Vokabularien durch die Verinigungsmenge beider Vokabularien geteilt wird. Die Metrik findet auch in  der englischen Bezeichnung Intersection over Union (IoU) verwendet. Je größer die Schnittmenge ist, um ähnlicher sind die Textkorpora und umso wahrscheinlicher stammen diese von einem:r Autor:in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_test(words_by_author, len_shortest_corpus):\n",
    "    \"\"\"Calculate Jaccard similarity of each known corpus to unknown corpus.\"\"\"\n",
    "    jaccard_by_author = dict()\n",
    "    unique_words_unknown = set(words_by_author['unknown']\n",
    "                               [:len_shortest_corpus])\n",
    "    authors = (author for author in words_by_author if author != 'unknown')    \n",
    "    for author in authors:\n",
    "        unique_words_author = set(words_by_author[author][:len_shortest_corpus]) \n",
    "        shared_words = unique_words_author.intersection(unique_words_unknown)\n",
    "        jaccard_sim = (float(len(shared_words))/ (len(unique_words_author) +\n",
    "                                                  len(unique_words_unknown) -\n",
    "                                                  len(shared_words)))\n",
    "        jaccard_by_author[author] = jaccard_sim\n",
    "        print('Jaccard Similarity für {} = {}'.format(author, jaccard_sim))\n",
    "        \n",
    "    most_likely_author = max(jaccard_by_author, key=jaccard_by_author.get)\n",
    "    print(f'Der wahrscheinlichste Autor nach Jaccard Similarity: {most_likely_author}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_test(words_by_author, len_shortest_corpus) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88af0ede569fe5c04062aeb490d0f03883bc9347bee31fbe08a2787a335dbcd5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
