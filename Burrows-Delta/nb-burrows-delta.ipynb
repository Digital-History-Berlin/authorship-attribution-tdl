{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Burrows Delta: Eine Methode des Machine Learning zur Detektion von Autorenschaft\n",
    "\n",
    "Ein bewährter stilometrischer Ansatz zur Erkennung von Autorenschaft ist die als Burrows Delta bezeichnete Methode, die mittels Machine Learning in einem überwachtem Verfahren mit gelabelten Trainingsdaten Texte von Autor:innen klassifiziert. Der dahinterstehende Algorithmus, der zu Beginn der 2000er-Jahre von John Burrows in die stilometrische Analyse eingeführt wurde, [^fn1] ist vergleichsweise einfach, liefert aber solide und fundierte Ergebnisse, insbesondere wenn längere Texte untersucht werden. Auch dieser Ansatz basiert auf der Berechnung von Distanzen zwischen den zu vergleichenden Texten. Im ersten Schritt werden mit den Autor:innennamen gelabelte Texte trainiert, um im zweiten Schritte neuen, unbekannten Texten das Autor:in-Label des Textes zuzuordnen, der stilistisch dem fraglichen Text am nächsten ist. Es wird also nach dem Prinzip des nächsten Nachbars (Nearest Neighbor) klassifiziert. Diese Form des Machine Learning wird auch als Instance-Based Learning oder Memory-Based Learning bezeichnet. \n",
    "\n",
    "Eine genaue und eingängige Beschreibung der mathematischen Herleitung von Burrows Delta bieten Karsdorp, Kestemont und Riddell in einem [Unterkapitel](https://www.humanitiesdataanalysis.org/stylometry/notebook.html#burrowss-delta) ihres einführenden Kapitels zur Stilometrie in den Digital Humanities: [Stylometry and the Voice of Hildegard](https://www.humanitiesdataanalysis.org/stylometry/notebook.html#). [^fn2]\n",
    "\n",
    "Aus diesem [Kapitel](https://www.humanitiesdataanalysis.org/stylometry/notebook.html#computing-document-distances-with-delta) sind der Code sowie die [Daten](https://doi.org/10.5281/zenodo.3560761) entnommen, [^fn3] die nachfolgend in leicht angepasster Fassung genutzt werden, um den Einsatz von Burrows Delta zur Zuschreibung von Autorschaft zu veranschaulichen. Als historisches Fallbeispiel dienen mittelalterliche Quellen, die entweder von Hildegard von Bingen oder von einem ihrer Sekretäre verfasst wurden. Ein informatives Video zu diesem Fall steht [hier](https://vimeo.com/70881172) zur Verfügung. Das Video wurde von den Forschenden erstellt, die diese unklare Lage bei der Autorenschaft der lateinischen Quellen aus dem 12. Jahrhundert wissenschaftlich untersucht haben. [^fn4]\n",
    "\n",
    "[^fn1]: John Burrows, 'Delta': a Measure of Stylistic Difference and a Guide to Likely Authorship, in: Literary and Linguistic Computing 17, Nr. 3 (2002), S. 267–287, https://doi.org/10.1093/llc/17.3.267.\n",
    "\n",
    "[^fn2]: Folgert Karsdorp, Mike Kestemont, Allen Roddell, Humanities Data Analysis. Case Studies with Python, Princeton 2021, S. 248-280, hier 252-254, https://www.humanitiesdataanalysis.org/index.html\n",
    "\n",
    "[^fn3]: Folgert Karsdorp, Mike Kestemont, Allen Roddell, Supplemental Materials for \"Humanities Data Analysis\" [Data set]. Zenodo, https://doi.org/10.5281/zenodo.3560761.\n",
    "\n",
    "[^fn4]: Mike Kestemont, Sara Moens, Jeroen Deploige, Collaborative authorship in the twelfth century: A stylometric study of Hildegard of Bingen and Guibert of Gembloux, in: Digital Scholarship in the Humanities 30, Nr. 2 (2015), S. 199–224, https://doi.org/10.1093/llc/fqt063"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import scipy.spatial.distance as scidist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helferfunktion\n",
    "\n",
    "### Zum Laden der Daten\n",
    "\n",
    "Im Ordner 'texts' befinden sich drei Textkorpora: Zum einen von Hildegard von Bingen, von ihrem Sekretär Guibert von Gembloux sowie als unabhängige Vergleichsgröße von einem weiteren Zeitgenossen, Bernard von Clairvaux. Die lateinischen Textkorpora liegen bereits in lemmatisierter Form vor. Da zur stilometrischen Analyse die zu vergleichenden Texte eine einheitliche Länge aufweisen sollten, bringt die nachfolgende Funktion die Texte auf den entsprechenden Umfang. Die Länge wird bestimmt vom kürzesten der zu untersuchenden Texte. Die im Unterordner 'test' abgelegten Dateien enthalten die zu prüfenden Quellen mit den unklaren Autor:innenzuschreibungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory(directory, max_length):\n",
    "    documents, authors, titles = [], [], []\n",
    "    for filename in os.scandir(directory):\n",
    "        if not filename.name.endswith('.txt'):\n",
    "            continue\n",
    "        author, _ = os.path.splitext(filename.name)\n",
    "\n",
    "        with open(filename.path) as f:\n",
    "            contents = f.read()\n",
    "        lemmas = contents.lower().split()\n",
    "        start_idx, end_idx, segm_cnt = 0, max_length, 1\n",
    "\n",
    "        # extract slices from the text:\n",
    "        while end_idx < len(lemmas):\n",
    "            documents.append(' '.join(lemmas[start_idx:end_idx]))\n",
    "            authors.append(author[0])\n",
    "            title = filename.name.replace('.txt', '').split('_')[1]\n",
    "            titles.append(f\"{title}-{segm_cnt}\")\n",
    "\n",
    "            start_idx += max_length\n",
    "            end_idx += max_length\n",
    "            segm_cnt += 1\n",
    "\n",
    "    return documents, authors, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen und Evaluieren von Burrows Delta auf dem bekannten Korpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden der Daten\n",
    "\n",
    "Die Textkorpora werden in Abschnitte (chunks) mit einer Länge von 10.000 Tokens gebracht. In den zurückgegebenen Listen befinden sich die Quellentexte als string (documents), der erste Buchstabe des:r Autor:in (authors) als Label für die spätere Klassifizierung sowie der restliche Teil des Dateinamens (title)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, authors, titles = load_directory('data/texts', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Dokumente: 36\n",
      "['B', 'B', 'B', 'B', 'B']\n",
      "['ep-1', 'ep-2', 'ep-3', 'ep-4', 'ep-5']\n"
     ]
    }
   ],
   "source": [
    "print(f'Anzahl der Dokumente: {len(documents)}')\n",
    "print(authors[:5])\n",
    "print(titles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen des Vokabulars und der Wordvektoren\n",
    "\n",
    "Computer verarbeiten Zahlen - daher müssen Texte in numerische Werte umgewandelt werden: Für jedes jedes Dokument in einem Textkorpus kann ein Vektor erstellt werden, der das Dokument repräsentiert. Bei stilometrischen Analysen wurde herausgefunden, dass Funktionswörter, insbesondere Stoppwörter, als starkes Signal zur Bestimmung von Autorenschaft herangezogen werden können. Funktionswörter werden von Autor:innen unbewusst in bestimmten Mustern genutzt, dadurch stellen sie eine sehr gute Signatur dar, um Autor:innen zu identifizieren. Für den vorliegenden Fall wurde eine Liste erstellt, die 65 Funktionswörter umfasst. Diese Liste wird zunächst aus der Datei wordlist.txt in die Variable vocab eingelesen. Dies ist das Vokabular, welches bei der weiteren stilometrischen Untersuchung zur Authorship Attribution genutzt wird. Mehr zum Hintergrund der Auswahl sowie zur Rolle von Funktionswörtern bietet ein [Unterkapitel](https://www.humanitiesdataanalysis.org/stylometry/notebook.html#function-words) in der Einführung Humanities Data Analysis von Karsdorp, Kestemont und Riddell.\n",
    "\n",
    "Die Python-Bibliothek [scikit-learn](https://scikit-learn.org/stable/#) bietet viele Funktionalitäten und Algorithmen für die Datenanalyse sowie für Machine Learning. Die Objektklasse [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer) ermöglicht es, die Häufigkeiten von Wörtern in einem Textkorpus zu ermitteln. Zugleich ermöglicht sie die Tokenisierung von Texten, die hier im nachfolgenden Code über einen als Parameter übergebenen regulären Ausdruck erfolgt. Im Ergebnis erstellt der CountVectorizer auf der Basis des übergebenen Vokabulars eine Matrix mit Wordhäufigkeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [l.strip() for l in open('data/wordlist.txt') if not l.startswith('#') and l.strip()][:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 65)\n",
      "['et' 'qui' 'in' 'non' 'ad']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = text.CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", vocabulary=vocab)\n",
    "v_documents = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "print(v_documents.shape)\n",
    "print(vectorizer.get_feature_names_out()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[347 369 170 289  87  94 100  66 105  59  91  86  45 108  24  32  53  24\n",
      "   51  26  49  23  38  27  17  12   9  24  15  20   6  34   3   6  10  37\n",
      "   15  10  30  23  13  20  18  12  21   3   2   2   8   7   2  10   9  13\n",
      "   16   7  12  11   1  10   3  12  12   3  16]\n",
      " [409 367 222 272  68 107 124  97  87  55  75  94  68  84  23  51  34  41\n",
      "   32  31  36  31  20  29  20  15  16  36  22  31  12  30   8  10  14  32\n",
      "    7   9  32  15  18  25  26  26  28   7   5  12  10   9   2   8   7  12\n",
      "    9   3   8   5   2   6   3   2  18   8   8]\n",
      " [392 335 206 203  84 126  87 110 109  53  65  86  76 113  29  31  34  38\n",
      "   31  78  42  21  40  19  20  18  15  29  25  22   8  36  21  18  22  37\n",
      "   12  10  39  27   7  14  10  19  10  11   1   6  10   9  14   8  13   7\n",
      "   11   4   8   5   2   6   5  15  17  12   7]\n",
      " [367 334 192 250  61  96 126 106 102  55  67 115  73  96  27  43  49  51\n",
      "   35  34  45  31  24  31  28   8  23  40  35  25   8  36  13  16  16  36\n",
      "   23  16  36  26   9  23  20  23  17   4  10   3  10   5   7  16  11   7\n",
      "   20   4  10  11   2   7   6  15  15   3   9]\n",
      " [343 363 198 229  83 125 125  92  78  79  65 101  67 102  22  36  55  27\n",
      "   55  46  62  42  33  27  27  14  14  24  21  11  10  28  11  17  21  38\n",
      "   16  13  42  22  21  19   9  13  13   3   6  12   4   9   8  14  10  12\n",
      "   13   9   7   3   1   8   3  14  10   4   6]]\n"
     ]
    }
   ],
   "source": [
    "print(v_documents[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisieren\n",
    "\n",
    "Beim Normalisieren wird jedes Element des Dokumentvektors durch die Länge des Vektors, d.h. durch die Summe aller Elemente des Vektors, geteilt. Zur Anwendung kommt wiederum eine [Methode zur Normalisierung](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html?highlight=normalize#sklearn.preprocessing.normalize) aus [scikit-learn](https://scikit-learn.org/stable/#), die im konkreten Fall mit L1 normalisiert, was der beschriebenen Division durch die Vektorlänge entspricht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstes Element aus dem ersten Dokumentenvektor: 347\n",
      "Summe aller Elemente des ersten Dokumentenvektors: 2877\n",
      "\n",
      "Normalisierter Wert des ersten Elements im ersten Dokumentenvektors: 0.12061174834897463\n"
     ]
    }
   ],
   "source": [
    "print(f'Erstes Element aus dem ersten Dokumentenvektor: {v_documents[0][0]}')\n",
    "print(f'Summe aller Elemente des ersten Dokumentenvektors: {v_documents[0].sum()}')\n",
    "print(f'\\nNormalisierter Wert des ersten Elements im ersten Dokumentenvektors: {v_documents[0][0] / v_documents[0].sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 65)\n"
     ]
    }
   ],
   "source": [
    "v_documents = preprocessing.normalize(v_documents.astype(float), norm='l1')\n",
    "\n",
    "print(v_documents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12061175, 0.1282586 , 0.05908933, 0.10045186, 0.03023983,\n",
       "        0.03267292, 0.03475843, 0.02294056, 0.03649635, 0.02050747,\n",
       "        0.03163017, 0.02989225, 0.01564129, 0.0375391 , 0.00834202,\n",
       "        0.0111227 , 0.01842197, 0.00834202, 0.0177268 , 0.00903719,\n",
       "        0.01703163, 0.00799444, 0.0132082 , 0.00938478, 0.00590893,\n",
       "        0.00417101, 0.00312826, 0.00834202, 0.00521376, 0.00695169,\n",
       "        0.00208551, 0.01181787, 0.00104275, 0.00208551, 0.00347584,\n",
       "        0.01286062, 0.00521376, 0.00347584, 0.01042753, 0.00799444,\n",
       "        0.0045186 , 0.00695169, 0.00625652, 0.00417101, 0.00729927,\n",
       "        0.00104275, 0.00069517, 0.00069517, 0.00278067, 0.00243309,\n",
       "        0.00069517, 0.00347584, 0.00312826, 0.0045186 , 0.00556135,\n",
       "        0.00243309, 0.00417101, 0.00382343, 0.00034758, 0.00347584,\n",
       "        0.00104275, 0.00417101, 0.00417101, 0.00104275, 0.00556135],\n",
       "       [0.13484998, 0.12100231, 0.07319486, 0.08968018, 0.02242005,\n",
       "        0.0352786 , 0.04088361, 0.03198154, 0.02868447, 0.01813386,\n",
       "        0.02472799, 0.03099242, 0.02242005, 0.02769535, 0.00758325,\n",
       "        0.01681503, 0.01121002, 0.01351797, 0.01055061, 0.0102209 ,\n",
       "        0.01186944, 0.0102209 , 0.00659413, 0.00956149, 0.00659413,\n",
       "        0.0049456 , 0.0052753 , 0.01186944, 0.00725354, 0.0102209 ,\n",
       "        0.00395648, 0.0098912 , 0.00263765, 0.00329707, 0.00461589,\n",
       "        0.01055061, 0.00230795, 0.00296736, 0.01055061, 0.0049456 ,\n",
       "        0.00593472, 0.00824266, 0.00857237, 0.00857237, 0.00923178,\n",
       "        0.00230795, 0.00164853, 0.00395648, 0.00329707, 0.00296736,\n",
       "        0.00065941, 0.00263765, 0.00230795, 0.00395648, 0.00296736,\n",
       "        0.00098912, 0.00263765, 0.00164853, 0.00065941, 0.00197824,\n",
       "        0.00098912, 0.00065941, 0.00593472, 0.00263765, 0.00263765]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_documents[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split\n",
    "\n",
    "Um die Genauigkeit des Klassifizierungsalgorithmus zu bestimmen, werden die bekannten Textdaten in ein Testdatenset und ein Trainigsdatenset aufgeteilt. Die Größe der Testdaten ist hier die doppelte Anzahl der Autoren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=6 Test Dokumente mit V=65 Features.\n",
      "N=30 Training Dokumente mit V=65 Features.\n"
     ]
    }
   ],
   "source": [
    "test_size = len(set(authors)) * 2\n",
    "\n",
    "(train_documents, test_documents,train_authors, test_authors) = model_selection.train_test_split(v_documents, \n",
    "                                                                                                 authors, \n",
    "                                                                                                 test_size=test_size, \n",
    "                                                                                                 stratify=authors, \n",
    "                                                                                                 random_state=42)\n",
    "                                                                                                 \n",
    "print(f'N={test_documents.shape[0]} Test Dokumente mit '\n",
    "      f'V={test_documents.shape[1]} Features.')\n",
    "\n",
    "print(f'N={train_documents.shape[0]} Training Dokumente mit '\n",
    "      f'V={train_documents.shape[1]} Features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Klassifikator\n",
    "\n",
    "In der nachfolgenden Python Objekt-Klasse wird ein Klassifikator erstellt, der die algorithmischen Berechnungen nach Vorgabe von Burrows Delta ausführt. Die Normalisierung der Vektoren ist bereits erfolgt. Zudem erfordert Burrows Delta eine Standardisierung. Die Klassenmethode *fit* übernimmt diesen Vorgang und nutzt dazu wiederum die Objektklasse [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler) aus scikit-learn. \n",
    "\n",
    "Die Klassenmethode *predict* berechnet die paarweisen Distanzen zwischen den Elementen zweier Sammlungen auf der Basis einer Distanzmetrik. Verwendung findet hier die [cdist()-Methode](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html) aus [SciPy](https://docs.scipy.org/doc/scipy/index.html#) einer weiteren Python-Bibliothek. Der voreingestellte Wert ist *cityblock*. Die Methode gibt als Vorhersage das Label (hier: den Autor) des Trainingsdokuments mit der geringsten Distanz zu dem Testdokument zurück, um auch das Testdokument mit diesem Label zu klassifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Delta:\n",
    "    \"\"\"Delta-Based Authorship Attributer.\"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit (or train) the attributer.\n",
    "\n",
    "        Arguments:\n",
    "            X: a two-dimensional array of size NxV, where N represents\n",
    "               the number of training documents, and V represents the\n",
    "               number of features used.\n",
    "            y: a list (or NumPy array) consisting of the observed author\n",
    "                for each document in X.\n",
    "\n",
    "        Returns:\n",
    "            Delta: A trained (fitted) instance of Delta.\n",
    "\n",
    "        \"\"\"\n",
    "        self.train_y = np.array(y)\n",
    "        self.scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "        self.train_X = self.scaler.fit_transform(X)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, metric='cityblock'):\n",
    "        \"\"\"Predict the authorship for each document in X.\n",
    "\n",
    "        Arguments:\n",
    "            X: a two-dimensional (sparse) matrix of size NxV, where N\n",
    "               represents the number of test documents, and V represents\n",
    "               the number of features used during the fitting stage of\n",
    "               the attributer.\n",
    "            metric (str, optional): the metric used for computing\n",
    "               distances between documents. Defaults to 'cityblock'.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: the predicted author for each document in X.\n",
    "\n",
    "        \"\"\"\n",
    "        X = self.scaler.transform(X)\n",
    "        dists = scidist.cdist(X, self.train_X, metric=metric)\n",
    "        return self.train_y[np.argmin(dists, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "\n",
    "Der Delta Klassifikator wird auf das Testsetting angewandt. Mit Hilfe einer bei scikit-learn verfügbaren Methode wird die [Accuracy-Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?highlight=accuracy#sklearn.metrics.accuracy_score) des Klassifikators ermittelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Autor ist B und B wurde vorhergesagt.\n",
      "Der Autor ist B und B wurde vorhergesagt.\n",
      "Der Autor ist B und B wurde vorhergesagt.\n",
      "Der Autor ist H und H wurde vorhergesagt.\n",
      "Der Autor ist G und G wurde vorhergesagt.\n",
      "Der Autor ist G und G wurde vorhergesagt.\n",
      "\n",
      "Accuracy der Vorhersagen: 1.0\n"
     ]
    }
   ],
   "source": [
    "delta = Delta()                             # Delta Classifier wird instantiiert\n",
    "delta.fit(train_documents, train_authors)   # Delta Classifier wird trainiert\n",
    "preds = delta.predict(test_documents)       # Delta Classifier klassifziert Test Dokumente\n",
    "\n",
    "# Erstellen der Print-Ausgabe\n",
    "for true, pred in zip(test_authors, preds):\n",
    "    _connector = 'ABER' if true != pred else 'und'\n",
    "    print(f'Der Autor ist {true} {_connector} {pred} wurde vorhergesagt.')\n",
    "\n",
    "accuracy = metrics.accuracy_score(preds, test_authors)\n",
    "print(f\"\\nAccuracy der Vorhersagen: {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorship Attribution: Anwendung auf den konkreten Untersuchungsfall\n",
    "\n",
    "### Einlesen der Trainingsdaten\n",
    "\n",
    "Die Länge des kürzesten zu prüfenden Quellendokuments ist 3.301 Tokens. Alle anderen Texte der Vergleichskorpora werden beim Einlesen auf diese Länge gebracht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents, train_authors, train_titles = load_directory('data/texts', 3301)\n",
    "\n",
    "vectorizer = text.CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", \n",
    "                                  vocabulary=vocab)\n",
    "                                  \n",
    "v_train_documents = vectorizer.fit_transform(train_documents).toarray()\n",
    "v_train_documents = preprocessing.normalize(v_train_documents.astype(float), norm='l1')\n",
    "\n",
    "delta = Delta().fit(v_train_documents, train_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Einlesen der Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs, test_authors, test_titles = load_directory('data/texts/test', 3301)\n",
    "\n",
    "v_test_docs = vectorizer.transform(test_docs).toarray()\n",
    "v_test_docs = preprocessing.normalize(v_test_docs.astype(float), norm='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klassifizieren mit Cityblock-Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quelle: Mart-1 => klassifiziert als B\n",
      "Quelle: Mart-1 => klassifiziert als G\n",
      "Quelle: Missa-1 => klassifiziert als G\n",
      "Quelle: Missa-2 => klassifiziert als G\n"
     ]
    }
   ],
   "source": [
    "predictions = delta.predict(v_test_docs)\n",
    "\n",
    "for filename, prediction in zip(test_titles, predictions):\n",
    "    print(f'Quelle: {filename} => klassifiziert als {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klassifizieren mit Cosinus-Metrik\n",
    "\n",
    "Die Klassifizierung unter Einbezug der Cosinus-Distanz ist bei stilometrischen Analysen zur Authorship Attribution ebenfalls solide und robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quelle: Mart-1 => klassifiziert als B\n",
      "Quelle: Mart-1 => klassifiziert als G\n",
      "Quelle: Missa-1 => klassifiziert als G\n",
      "Quelle: Missa-2 => klassifiziert als G\n"
     ]
    }
   ],
   "source": [
    "predictions = delta.predict(v_test_docs, metric='cosine')\n",
    "\n",
    "for filename, prediction in zip(test_titles, predictions):\n",
    "    print(f'Quelle: {filename} => klassifiziert als {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnis\n",
    "\n",
    "Die Untersuchung mit Burrows Delta hat ergeben, dass keine der untersuchten Quellen eine Autorenschaft von Hildegard von Bingen nahelegt. Stattdessen kommt eher eine Autorenschaft von Guibert von Gembloux in Frage."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88af0ede569fe5c04062aeb490d0f03883bc9347bee31fbe08a2787a335dbcd5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
